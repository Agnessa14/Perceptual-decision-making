{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from rcnn_sat import preprocess_image\n",
    "import os\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.decomposition import PCA as RandomizedPCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import entropy\n",
    "from scipy import stats\n",
    "import scipy.io as sio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and preprocess training & validation data\n",
    "#training\n",
    "with open('2400_selected_scenes_places365_train_standard.json') as json_file:\n",
    "    subset_scenes_dict_train = json.load(json_file)  \n",
    "train_set_path = '/scratch/agnek95/PDM/places_365_256_train_val/data_256/'\n",
    "train_image_paths = list(subset_scenes_dict_train.keys())\n",
    "train_imgs_prep = np.ones([len(train_image_paths),128,128,3])\n",
    "train_imgs_prep[:] = np.nan\n",
    "for idx,image_path in enumerate(train_image_paths):\n",
    "        image = load_img(train_set_path+image_path, target_size=(128, 128)) \n",
    "        image = img_to_array(image)\n",
    "        image = np.uint8(image)\n",
    "        image = preprocess_image(image)\n",
    "        train_imgs_prep[idx,:,:,:] = image\n",
    "        \n",
    "train_images_paths = [train_set_path+image_path for image_path in train_image_paths]\n",
    "\n",
    "#validation    \n",
    "with open('1200_selected_scenes_places365_val_standard.json') as json_file:\n",
    "    subset_scenes_dict_val = json.load(json_file)  \n",
    "val_set_path = '/scratch/agnek95/PDM/places_365_256_train_val/val_256/' \n",
    "val_image_paths = list(subset_scenes_dict_val.keys())\n",
    "val_imgs_prep = np.ones([len(val_image_paths),128,128,3])\n",
    "val_imgs_prep[:] = np.nan\n",
    "for idx,image_path in enumerate(val_image_paths):\n",
    "        image = load_img(val_set_path+image_path, target_size=(128, 128)) \n",
    "        image = img_to_array(image)\n",
    "        image = np.uint8(image)\n",
    "        image = preprocess_image(image)\n",
    "        val_imgs_prep[idx,:,:,:] = image\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '/home/agnek95/SMST/PDM_PILOT_2/ANALYSIS_Full_experiment/DNN/'\n",
    "data_path = '/scratch/agnek95/PDM/places365/val_large'\n",
    "#IDs of the scenes of interest\n",
    "with open(os.path.join(main_path,'scenes_eeg_ordered.json')) as json_file:\n",
    "    scenes_60 = json.load(json_file)      \n",
    "         \n",
    "selected_scenes = list(scenes_60.keys())\n",
    "test_images_paths = [None]*len(selected_scenes)\n",
    "for index,file in enumerate(selected_scenes):\n",
    "    test_images_paths[index] = os.path.join(data_path,selected_scenes[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels: man-made: 0, natural: 1\n",
    "y_train = np.array([label for label in range(2) for reps in range(int(train_imgs_prep.shape[0]/2))]) \n",
    "y_val = np.array([label for label in range(2) for reps in range(int(val_imgs_prep.shape[0]/2))])\n",
    "\n",
    "x_train =  train_imgs_prep\n",
    "x_val = val_imgs_prep\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the network graph with readouts (one readout at a time), load weights from previously fine-tuned network (on scenes) w/o readouts and collect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_readout(model):\n",
    "    base_learning_rate = 0.0001 #in case you want to use optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        shuffle=True,\n",
    "        batch_size=10,\n",
    "        epochs=20, \n",
    "        validation_data=(x_val,y_val)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_predictions(model,num_images_all, num_classes, batch_size):\n",
    "    num_batches = int(num_images_all / batch_size)\n",
    "    pred = np.ones([num_batches,batch_size,num_classes])\n",
    "    pred[:] = np.nan\n",
    "\n",
    "    #preprocess images\n",
    "    for batch, img_idx in enumerate(range(0, num_images_all, batch_size)):\n",
    "        batch_paths = test_images_paths[img_idx:img_idx + batch_size] \n",
    "        batch_images = np.zeros((batch_size,128,128,3)) \n",
    "        for i, image_path in enumerate(batch_paths):\n",
    "            image = load_img(image_path, target_size=(128, 128)) \n",
    "            image = img_to_array(image)\n",
    "            image = np.uint8(image)\n",
    "            image = preprocess_image(image)\n",
    "            batch_images[i,:,:,:] = image\n",
    "\n",
    "        #predictions\n",
    "        pred[batch,:,:] = model(batch_images)\n",
    "\n",
    "    #reshape: all images from all batches in one dimension\n",
    "    # pred_reshaped =  np.transpose(pred,(0,2,1,3)).reshape(num_batches*batch_size,num_classes)\n",
    "    pred_reshaped =  pred.reshape(num_batches*batch_size,num_classes)\n",
    "    \n",
    "    return pred_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_trainable(model: tf.keras.Model):\n",
    "   for layer in model.layers:\n",
    "            if 'Readout' in layer.name:\n",
    "                layer._trainable = True\n",
    "            else:\n",
    "                layer._trainable = False\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One readout at a time - load the weights of the other layers, train the readout and get the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'b_d' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rcnn_sat import b_d_net_readout_0, b_d_net_readout_1, b_d_net_readout_2, b_d_net_readout_3, b_d_net_readout_4, b_d_net_readout_5, b_d_net_readout_final\n",
    "\n",
    "input_layer = tf.keras.layers.Input((128, 128, 3))\n",
    "weights_name='{}_net_scenes_weights.h5'.format(model_type)\n",
    "num_readouts=7\n",
    "num_classes=2\n",
    "num_images_all = len(test_images_paths)\n",
    "all_models = [b_d_net_readout_0, b_d_net_readout_1, b_d_net_readout_2, b_d_net_readout_3, b_d_net_readout_4, b_d_net_readout_5, b_d_net_readout_final]\n",
    "# all_models = [b_d_net_readout_0, b_d_net_readout_1]\n",
    "predictions_all_readouts = np.zeros([num_readouts,num_images_all,num_classes])\n",
    "predictions_all_readouts[:] = np.nan\n",
    "\n",
    "for m,readout_model in enumerate(all_models):\n",
    "    tf.keras.backend.clear_session() \n",
    "    \n",
    "    model = readout_model(input_layer,classes_scenes=2)\n",
    "    non_trainable(model)\n",
    "    # tf.keras.backend.set_floatx('float64') #To change all layers to have dtype float64 by default - warning from TF\n",
    "    # print(model.summary())\n",
    "    \n",
    "    #Load weights\n",
    "    model.load_weights(weights_name,by_name=True)\n",
    "    \n",
    "    #train the readout\n",
    "    train_readout(model)\n",
    "\n",
    "    #save weights\n",
    "    model_name='{}_net_scenes_readout_{}'.format(model_type,m)\n",
    "    model.save_weights(model_name+'_weights.h5')\n",
    "\n",
    "    #collect predictions\n",
    "    batch_size = 60\n",
    "    predictions_all_readouts[m,:,:]=collect_predictions(model,num_images_all, num_classes, batch_size)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get RTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RTs(entropy_thresh):\n",
    "    #get entropies for each image & each timepoint\n",
    "    entropies_pred = np.ones([num_readouts, num_images_all])\n",
    "    entropies_pred[:] = np.nan\n",
    "\n",
    "    for readout in range(num_readouts):\n",
    "        for image in range(num_images_all):\n",
    "            entropies_pred[readout,image] = entropy(predictions_all_readouts[readout,image,:])\n",
    "\n",
    "    #for each image, determine the timepoint when entropy reaches threshold\n",
    "    rt_thresh = np.ones(num_images_all)\n",
    "    rt_thresh[:] = np.nan\n",
    "    \n",
    "    for image in range(num_images_all):\n",
    "        for readout in range(num_readouts):\n",
    "            if entropies_pred[readout,image] <= entropy_thresh:\n",
    "                rt_thresh[image]=readout\n",
    "                break          \n",
    "\n",
    "    #if it never reaches the threshold (nan in the array), replace by 8\n",
    "    rt_thresh[np.isnan(rt_thresh)] = 8\n",
    "   \n",
    "    return rt_thresh,entropies_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load RTs\n",
    "rts_eeg_dict = sio.loadmat('RT_all_subjects_5_35_categorization.mat')\n",
    "rts_eeg = rts_eeg_dict.get('RTs')\n",
    "\n",
    "#define some variables\n",
    "num_subjects = rts_eeg.shape[0]\n",
    "entropies = np.arange(0.01,0.1,0.01)\n",
    "best_entropy = np.ones([num_subjects])\n",
    "best_entropy[:] = np.nan\n",
    "correlation_test = np.ones([num_subjects,3]) #all,artificial,natural\n",
    "correlation_test[:] = np.nan\n",
    "num_scenes = len(test_images_paths)\n",
    "\n",
    "#get RNN RTs for every entropy threshold and correlate with humans\n",
    "rts_dnn = np.ones([len(entropies),len(test_images_paths)])\n",
    "rts_dnn[:] = np.nan\n",
    "\n",
    "for idx,e in enumerate(entropies):\n",
    "    rts_dnn[idx,:],entropies_pred = get_RTs(e)\n",
    "    \n",
    "    \n",
    "#for each fold, fit the entropy threshold on 29 subjects\n",
    "for s in range(num_subjects): \n",
    "    artificial_idx = np.arange(30)\n",
    "    natural_idx = np.arange(30,60)\n",
    "\n",
    "    test_sub = rts_eeg[s,:]\n",
    "    fit_sub = np.nanmean(rts_eeg[np.arange(num_subjects)!=s,:],0)\n",
    "    correlation_fit = np.ones([len(entropies),2])\n",
    "    correlation_fit[:] = np.nan\n",
    "\n",
    "    for idx,e in enumerate(entropies):\n",
    "        correlation_fit[idx,0] = stats.pearsonr(np.squeeze(rts_rnn[idx,artificial_idx]),fit_sub[artificial_idx])[0] #artificial\n",
    "        correlation_fit[idx,1] = stats.pearsonr(np.squeeze(rts_rnn[idx,natural_idx]),fit_sub[natural_idx])[0] #natural\n",
    "        \n",
    "    #select the entropy with highest mean (over scene conditions) correlation\n",
    "    best_entropy[s] = round(entropies[np.nanargmax(np.mean(correlation_fit,1))],2)     \n",
    "    idx_best_entropy=np.nanargmax(np.mean(correlation_fit,1))\n",
    "    \n",
    "    #remove scene if there's no RT for it \n",
    "    selected_dnn_rts = rts_dnn[idx_best_entropy,:]\n",
    "    if np.argwhere(np.isnan(test_sub)).size:\n",
    "        print(s)\n",
    "        removed_scene = np.argwhere(np.isnan(test_sub))[0][0]\n",
    "        if removed_scene in natural_idx:\n",
    "            natural_idx = np.delete(natural_idx,removed_scene-30)\n",
    "        elif removed_scene in artificial_idx:\n",
    "            artificial_idx = np.delete(artificial_idx,removed_scene)\n",
    "\n",
    "    #correlate with leftout subject        \n",
    "    correlation_test[s,0] = stats.pearsonr(selected_dnn_rts[np.concatenate((artificial_idx,natural_idx))],\\\n",
    "                                           test_sub[np.concatenate((artificial_idx,natural_idx))])[0]        \n",
    "    correlation_test[s,1] = stats.pearsonr(selected_dnn_rts[artificial_idx],test_sub[artificial_idx])[0]\n",
    "    correlation_test[s,2] = stats.pearsonr(selected_dnn_rts[natural_idx],test_sub[natural_idx])[0]\n",
    "    \n",
    "print(best_entropy)\n",
    "print(np.mean(correlation_test,0))\n",
    "RT_entropy = stats.mode(best_entropy)[0][0] #which should hopefully be the same for every fold anyway - idx_best_entropy\n",
    "np.save('correlation_RT_human_{}_net_7_layers_cross-validated_readouts'.format(model_type),correlation_test)\n",
    "np.save('{}_net_RTs_7_layers_entropy_threshold_readouts_{}'.format(model_type,RT_entropy),selected_dnn_rts)\n",
    "\n",
    "#save as mat files\n",
    "sio.savemat('correlation_RT_human_{}_net_7_layers_cross-validated_readouts.mat'.format(model_type),dict(data=correlation_test))\n",
    "sio.savemat('{}_net_RTs_7_layers_entropy_threshold_readouts_{}.mat'.format(model_type,RT_entropy),dict(data=selected_dnn_rts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save weights\n",
    "# model_name='{}_net_scenes_readout_{}'.format(model_type,m)\n",
    "# print(model_name)\n",
    "# model.save_weights(model_name+'_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rcnn_sat import b_d_net_readouts\n",
    "# input_layer = tf.keras.layers.Input((128, 128, 3))\n",
    "# model = b_d_net_readouts(input_layer, classes_scenes=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the weights of all layers but the readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_name='{}_net_scenes_weights.h5'.format(model_type)\n",
    "# model.load_weights(weights_name,by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rcnn_sat import b_d_net_readout_final\n",
    "# model = b_d_net_readout_final(input_layer, classes_scenes=2)\n",
    "# model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define trainable layers (readouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model.layers:\n",
    "#     if 'Readout' in layer.name:\n",
    "#         layer.trainable = True\n",
    "#     else:\n",
    "#         layer.trainable = False\n",
    "               \n",
    "# for layer in model.layers:\n",
    "#     if layer.trainable==True:\n",
    "#         print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_readouts=7\n",
    "# x_train_all = np.repeat(x_train[None,...],7,axis=0)\n",
    "# y_train_all = np.tile(y_train,(num_readouts,1))\n",
    "\n",
    "# x_val_all = np.repeat(x_val[None,...],7,axis=0)\n",
    "# y_val_all = np.tile(y_val,(num_readouts,1))\n",
    "\n",
    "# print(x_train_all.shape)\n",
    "# print(y_train_all.shape)\n",
    "# print(x_val_all.shape)\n",
    "# print(y_val_all.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_learning_rate = 0.0001 #in case you want to use optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate)\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(\n",
    "#     (x_train,x_train,x_train,x_train,x_train,x_train,x_train),\n",
    "#     (y_train,y_train,y_train,y_train,y_train,y_train,y_train),\n",
    "#     shuffle=True,\n",
    "#     batch_size=10,\n",
    "#     epochs=20, #5?\n",
    "#     validation_data=((x_val,x_val,x_val,x_val,x_val,x_val,x_val), (y_val,y_val,y_val,y_val,y_val,y_val,y_val)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(x_val, batch_size = 10)\n",
    "# print(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name='{}_net_scenes_readouts'.format(model_type)\n",
    "# model.save_weights(model_name+'_weights.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you want to get the RTs rightaway, follow the next steps, otherwise use the separate script (rnn_dth_collect_activations.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_RTs(test_images_path,batch_size,entropy_thresh):\n",
    "#     num_images_all = len(test_images_path)\n",
    "#     num_batches = int(num_images_all / batch_size)\n",
    "#     num_timepoints = 8\n",
    "#     num_classes = 2\n",
    "#     all_batches_activ = np.ones([num_batches, batch_size, 128, 128, 3])\n",
    "#     all_batches_activ[:] = np.nan\n",
    "#     pred = np.ones([num_batches,num_timepoints,batch_size,num_classes])\n",
    "#     pred[:] = np.nan\n",
    "\n",
    "#     for batch, img_idx in enumerate(range(0, num_images_all, batch_size)):\n",
    "#         batch_paths = test_images_path[img_idx:img_idx + batch_size] \n",
    "#         batch_images = np.zeros((batch_size,128,128,3)) \n",
    "#         for i, image_path in enumerate(batch_paths):\n",
    "#             image = load_img(image_path, target_size=(128, 128)) \n",
    "#             image = img_to_array(image)\n",
    "#             image = np.uint8(image)\n",
    "#             image = preprocess_image(image)\n",
    "#             batch_images[i,:,:,:] = image\n",
    "\n",
    "#         #predictions\n",
    "#         pred[batch,:,:,:] = model(batch_images) #shape: num_timepoints x batch_size x classes\n",
    "\n",
    "#     #reshape: all images from all batches in one dimension\n",
    "#     pred_reshaped =  np.transpose(pred,(0,2,1,3)).reshape(num_batches*batch_size,num_timepoints,num_classes)\n",
    "\n",
    "    #get entropies for each image & each timepoint\n",
    "#     entropies_pred = np.ones([num_images_all,num_timepoints])\n",
    "#     entropies_pred[:] = np.nan\n",
    "\n",
    "#     for image in range(num_images_all):\n",
    "#         for tp in range(num_timepoints):\n",
    "#             entropies_pred[image,tp] = entropy(pred_reshaped[image,tp])\n",
    "\n",
    "#     # #for each image, determine the timepoint when entropy reaches threshold\n",
    "#     rt_thresh = np.ones(num_images_all)\n",
    "#     rt_thresh[:] = np.nan\n",
    "#     for image in range(num_images_all):\n",
    "#         for tp in range(num_timepoints):\n",
    "#             if entropies_pred[image,tp] <= entropy_thresh:\n",
    "#                 rt_thresh[image]=tp\n",
    "#                 break          \n",
    "\n",
    "#     #if it never reaches the threshold (nan in the array), replace by 8\n",
    "#     rt_thresh[np.isnan(rt_thresh)] = 8\n",
    "   \n",
    "#     return rt_thresh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNN_env_kernel",
   "language": "python",
   "name": "dnn_env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
