{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "# import scipy as sp\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA as RandomizedPCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load RTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_dir = 'model_02.11_2'\n",
    "rnn_path = '/mnt/raid/ni/agnessa/rcnn-sat/'\n",
    "path_rt = rnn_path+model_dir\n",
    "entropy_thresh = 0.07\n",
    "rts = np.load(os.path.join(path_rt,'reaction_time_entropy_th_{}.npy'.format(entropy_thresh)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_svm(x_train,y_train,x_test):\n",
    "    clf = svm.SVC(kernel='linear', C=1000)\n",
    "    print('Training')\n",
    "    clf.fit(x_train, y_train) \n",
    "    print('Evaluating')\n",
    "    distances = clf.decision_function(x_test)\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define x train\n",
    "# for a specific layer and tp:\n",
    "#  load the num_images x num_features activations vector\n",
    "#  this is your x_train\n",
    "#  del the activations variable\n",
    "\n",
    "all_activ_path = rnn_path+model_dir+'/normalized_nopca/activations'\n",
    "size_train = 2400\n",
    "size_test = 60\n",
    "num_ps = 4 #num pseudoscenes - bins of scenes\n",
    "num_scenes_ps = size_train/num_ps \n",
    "y_train_labels = [label for label in range(2) for reps in range(int(num_ps/2))] \n",
    "num_layers = 7\n",
    "num_timepoints = 8\n",
    "distances = np.ones([num_layers,num_timepoints,size_test])\n",
    "distances[:] = np.nan\n",
    "dth_rt_corr = np.ones([num_layers,num_timepoints])\n",
    "dth_rt_corr_art = np.ones([num_layers,num_timepoints])\n",
    "dth_rt_corr_nat = np.ones([num_layers,num_timepoints])\n",
    "dth_rt_corr[:] = np.nan\n",
    "dth_rt_corr_art[:] = np.nan\n",
    "dth_rt_corr_nat[:] = np.nan\n",
    "\n",
    "for layer_idx in range(num_layers):\n",
    "    for timepoint in range(num_timepoints):\n",
    "        layer_time = 'ReLU_Layer_{}_Time_{}'.format(layer_idx,timepoint)\n",
    "#             x_test_pca = np.load(os.path.join(test_activ_path_pca, '{}_activations_pca.npy'.format(layer_time)))\n",
    "        activs = np.load(os.path.join(all_activ_path,'{}_activations.npy'.format(layer_time)))\n",
    "        x_train = activs[0:size_train,:]\n",
    "        print('Shape train:',x_train.shape)\n",
    "        x_test = activs[size_train:,:]\n",
    "        print('Shape test:',x_test.shape)\n",
    "\n",
    "        #group into pseudotrials\n",
    "        pseudoscenes_x_train = np.ones([4,x_train.shape[1]])\n",
    "        pseudoscenes_x_train[:] = np.nan\n",
    "        for ps in range(4):\n",
    "            pseudoscenes_x_train[ps] = np.mean(x_train[ps*300:(ps*300)+300,:])\n",
    "        print('Finished grouping')\n",
    "        print(pseudoscenes_x_train.shape[:])\n",
    "\n",
    "        #SVM: get distances\n",
    "        distances[layer_idx,timepoint,:] = abs(run_svm(pseudoscenes_x_train,y_train_labels,x_test))\n",
    "        print('Finished SVM')\n",
    "\n",
    "        #run the correlation with RTs\n",
    "        dth_rt_corr[layer_idx,timepoint] = stats.spearmanr(np.squeeze(distances[layer_idx,timepoint,:]),rts)[0]\n",
    "        dth_rt_corr_art[layer_idx,timepoint] = stats.spearmanr(np.squeeze(distances[layer_idx,timepoint,0:30]),rts[0:30])[0]\n",
    "        dth_rt_corr_nat[layer_idx,timepoint] = stats.spearmanr(np.squeeze(distances[layer_idx,timepoint,30:]),rts[30:])[0]\n",
    "        print('Finished correlation')  \n",
    "        print('Correlation', dth_rt_corr[layer_idx,timepoint])\n",
    "        print('Correlation', dth_rt_corr_art[layer_idx,timepoint])\n",
    "        print('Correlation', dth_rt_corr_nat[layer_idx,timepoint])\n",
    "\n",
    "\n",
    "# save DTH results        \n",
    "path_dth_results = os.path.join(rnn_path+model_dir+'/normalized_nopca/','correlation_rt_{}_dth'.format(entropy_thresh))\n",
    "path_dth_results_art = os.path.join(rnn_path+model_dir+'/normalized_nopca/','correlation_rt_{}_dth_art'.format(entropy_thresh))\n",
    "path_dth_results_nat = os.path.join(rnn_path+model_dir+'/normalized_nopca/','correlation_rt_{}_dth_nat'.format(entropy_thresh))\n",
    "path_distances = os.path.join(rnn_path+model_dir+'/normalized_nopca/','distances_all_scenes'.format(entropy_thresh))\n",
    "\n",
    "np.save(path_dth_results,dth_rt_corr)\n",
    "np.save(path_dth_results_art,dth_rt_corr_art)\n",
    "np.save(path_dth_results_nat,dth_rt_corr_nat)\n",
    "np.save(path_distances,distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#all scenes\n",
    "num_layers = 7\n",
    "cm = plt.get_cmap('OrRd')\n",
    "colors = cm(np.linspace(0,1,num_layers))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "for layer in range(num_layers):\n",
    "    ax.plot(dth_rt_corr[layer,:],label='Layer {}'.format(layer),color=colors[layer])\n",
    "    \n",
    "ax.set(xlabel='Timepoint', ylabel='Spearman\\'s r',\n",
    "   title='Correlation between distance to hyperplane and RT (all scenes)')\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "figname_svg = os.path.join(rnn_path+model_dir+'/normalized_nopca/','dth_rt_{}_corr_all_scenes.svg'.format(entropy_thresh))\n",
    "figname_png = os.path.join(rnn_path+model_dir+'/normalized_nopca/','dth_rt_{}_corr_all_scenes.png'.format(entropy_thresh))\n",
    "\n",
    "fig.savefig(figname_svg,bbox_inches='tight') #to make sure the legend is not cut off \n",
    "fig.savefig(figname_png,bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#artificial scenes\n",
    "\n",
    "cm = plt.get_cmap('PuRd')\n",
    "colors = cm(np.linspace(0,1,num_layers))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "for layer in range(num_layers):\n",
    "    ax.plot(dth_rt_corr_art[layer,:],label='Layer {}'.format(layer),color=colors[layer])\n",
    "    \n",
    "ax.set(xlabel='Timepoint', ylabel='Spearman\\'s r',\n",
    "   title='Correlation between distance to hyperplane and RT (artificial scenes)')\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "figname_svg = os.path.join(rnn_path+model_dir+'/normalized_nopca/','dth_rt_{}_corr_artificial.svg'.format(entropy_thresh))\n",
    "figname_png = os.path.join(rnn_path+model_dir+'/normalized_nopca/','dth_rt_{}_corr_artificial.png'.format(entropy_thresh))\n",
    "\n",
    "fig.savefig(figname_svg,bbox_inches='tight') #to make sure the legend is not cut off \n",
    "fig.savefig(figname_png,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#natural scenes\n",
    "\n",
    "cm = plt.get_cmap('YlGn')\n",
    "colors = cm(np.linspace(0,1,num_layers))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "for layer in range(num_layers):\n",
    "    ax.plot(dth_rt_corr_nat[layer,:],label='Layer {}'.format(layer),color=colors[layer])\n",
    "    \n",
    "ax.set(xlabel='Timepoint', ylabel='Spearman\\'s r',\n",
    "   title='Correlation between distance to hyperplane and RT (natural scenes)')\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "figname_svg = os.path.join(rnn_path+model_dir+'/normalized_nopca/','dth_rt_{}_corr_natural.svg'.format(entropy_thresh))\n",
    "figname_png = os.path.join(rnn_path+model_dir+'/normalized_nopca/','dth_rt_{}_corr_natural.png'.format(entropy_thresh))\n",
    "\n",
    "fig.savefig(figname_svg,bbox_inches='tight') #to make sure the legend is not cut off \n",
    "fig.savefig(figname_png,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#If the load function gives an error, do this\n",
    "# np_load_old = np.load # modify the default parameters of np.load\n",
    "# np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "# t = np.load('model_06.08/ReLU_Layer_1_Time_1_activations.npy') #test\n",
    "# np.load = np_load_old"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNN_env_kernel",
   "language": "python",
   "name": "dnn_env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
