{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from rcnn_sat_2 import preprocess_image, bl_net\n",
    "import os\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.decomposition import PCA as RandomizedPCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import entropy\n",
    "from scipy import stats\n",
    "import scipy.io as sio\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore fine-tuned model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Input((128, 128, 3))\n",
    "model = bl_net(input_layer, classes_scenes=2, cumulative_readout=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_02.11_2'\n",
    "model.load_weights(os.path.join(model_name,model_name+'_weights.h5')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training & testing data for the SVM model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) create json files of names+labels of selected training/validation data (5 per class for training, 2 per class for val)\n",
    "#   -- not all classes shown in txt file... don't need all i think. so just pick 5 from 200 classes\n",
    "#2) upload the training data onto eltanin\n",
    "#3) run through "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and preprocess training data\n",
    "data_names_path = '/mnt/raid/ni/agnessa/rcnn-sat/'\n",
    "\n",
    "#training\n",
    "with open(os.path.join(data_names_path,'2400_selected_scenes_places365_train_standard.json')) as json_file:\n",
    "    subset_scenes_dict_train = json.load(json_file)  \n",
    "train_set_path = '/mnt/raid/data/agnessa/data_256'\n",
    "train_paths = list(subset_scenes_dict_train.keys())\n",
    "train_imgs_prep = np.ones([len(train_paths),128,128,3])\n",
    "train_imgs_prep[:] = np.nan\n",
    "for idx,image_path in enumerate(train_paths):\n",
    "        image = load_img(train_set_path+image_path, target_size=(128, 128)) \n",
    "        image = img_to_array(image)\n",
    "        image = np.uint8(image)\n",
    "        image = preprocess_image(image)\n",
    "        train_imgs_prep[idx,:,:,:] = image\n",
    "    \n",
    "# #define y (labels) and x    \n",
    "y_train = np.array([label for label in range(2) for reps in range(int(train_imgs_prep.shape[0]/2))]) \n",
    "x_train =  train_imgs_prep\n",
    "\n",
    "#all paths together\n",
    "train_images_paths = [train_set_path+image_path for image_path in train_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '/mnt/raid/ni/agnessa'\n",
    "data_path = '/mnt/raid/data/agnessa/val_256'\n",
    "\n",
    "#IDs of the scenes of interest\n",
    "with open(os.path.join(main_path,'RSA/RSA_EEG/scenes_eeg_ordered.json')) as json_file:\n",
    "    scenes_60 = json.load(json_file)      \n",
    "         \n",
    "selected_scenes = list(scenes_60.keys())\n",
    "test_images_paths = [None]*len(selected_scenes)\n",
    "for index,file in enumerate(selected_scenes):\n",
    "    test_images_paths[index] = os.path.join(data_path,selected_scenes[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_pca(activations,num_components):\n",
    "#     np.random.seed(0)   \n",
    "#     scaler = MinMaxScaler(feature_range=[0 ,1]) #normalize\n",
    "#     activs_scaled = scaler.fit_transform(activations)\n",
    "#     pca = RandomizedPCA(n_components = num_components)\n",
    "#     pca_results = pca.fit_transform(activs_scaled)\n",
    "    \n",
    "#     return pca_results, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get activations from all layers & all timepoints (train+test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(img_paths,batch_size,model_directory):\n",
    "\n",
    "    ## preallocate\n",
    "    num_images_all = len(img_paths)\n",
    "    num_batches = int(num_images_all / batch_size)\n",
    "    num_layers = 7\n",
    "    num_timepoints = 8\n",
    "\n",
    "    ## loop over all layers and timepoints\n",
    "    for layer_idx in range(num_layers):\n",
    "        for timepoint in range(num_timepoints):\n",
    "            activ = []\n",
    "            layer_time =  'ReLU_Layer_{}_Time_{}'.format(layer_idx,timepoint)\n",
    "            get_layer_activation = tf.keras.backend.function(\n",
    "            [model.input],\n",
    "            [model.get_layer(layer_time).output])\n",
    "            for batch, img_idx in enumerate(range(0, num_images_all, batch_size)):\n",
    "                print('Getting activations for layer',layer_idx,', timepoint',timepoint,', batch',batch)\n",
    "                batch_paths = img_paths[img_idx:img_idx + batch_size] \n",
    "                batch_images = np.zeros((batch_size,128,128,3)) \n",
    "                #preprocessing images\n",
    "                for i, image_path in enumerate(batch_paths):\n",
    "                    image = load_img(image_path, target_size=(128, 128)) \n",
    "                    image = img_to_array(image)\n",
    "                    image = np.uint8(image)\n",
    "                    image = preprocess_image(image)\n",
    "                    batch_images[i,:,:,:] = image\n",
    "\n",
    "                activ.append(list(np.array(get_layer_activation(batch_images)).squeeze()))  \n",
    "            \n",
    "#           flatten the vector of activations from all batches into (num-all-images x num-all-features)    \n",
    "            flattened_activ = np.array(activ).reshape(num_images_all,-1)\n",
    "\n",
    "            #replace zeros\n",
    "            flattened_activ[flattened_activ==0]=0.000001\n",
    "            \n",
    "            #z-score across images\n",
    "            zscored_activ = stats.zscore(flattened_activ,axis=0) #normalize over images\n",
    "            print(np.mean(zscored_activ[:,0])) #get the mean of feature 1 values for all imgs - should be 0\n",
    "#             print(zscored_activ[0,82])\n",
    "#             print(zscored_activ[0,12113])\n",
    "#             print(zscored_activ[0,12209])\n",
    "            #pca\n",
    "#             [pca_activs,pca_object] = run_pca(zscored_activ,num_components_pca)\n",
    "#             print('Variance explained:',pca_object.explained_variance_ratio_)\n",
    "            path = os.path.join(main_path + '/rcnn-sat/'+model_directory, '{}_activations'.format(layer_time))\n",
    "            np.save(path,zscored_activ)\n",
    "            print('Saved zscored activations')\n",
    "\n",
    "            del flattened_activ\n",
    "            del activ\n",
    "            del zscored_activ\n",
    "#             del pca_activs\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# activations_directory = model_name+'/normalized_pca_1000/activations/'\n",
    "# activations_directory = model_name+'/normalized_nopca/activations/'\n",
    "\n",
    "t = train_images_paths+test_images_paths\n",
    "# get_activations(t,20,activations_directory)\n",
    "\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get RTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RTs(test_images_path,batch_size,entropy_thresh):\n",
    "    num_images_all = len(test_images_path)\n",
    "    num_batches = int(num_images_all / batch_size)\n",
    "    num_timepoints = 8\n",
    "    num_classes = 2\n",
    "    all_batches_activ = np.ones([num_batches, batch_size, 128, 128, 3])\n",
    "    all_batches_activ[:] = np.nan\n",
    "    pred = np.ones([num_batches,num_timepoints,batch_size,num_classes])\n",
    "    pred[:] = np.nan\n",
    "\n",
    "    for batch, img_idx in enumerate(range(0, num_images_all, batch_size)):\n",
    "        batch_paths = test_images_path[img_idx:img_idx + batch_size] \n",
    "        batch_images = np.zeros((batch_size,128,128,3)) \n",
    "        for i, image_path in enumerate(batch_paths):\n",
    "            image = load_img(image_path, target_size=(128, 128)) \n",
    "            image = img_to_array(image)\n",
    "            image = np.uint8(image)\n",
    "            image = preprocess_image(image)\n",
    "            batch_images[i,:,:,:] = image\n",
    "\n",
    "        #predictions\n",
    "        pred[batch,:,:,:] = model(batch_images) #shape: num_timepoints x batch_size x classes\n",
    "\n",
    "    #reshape: all images from all batches in one dimension\n",
    "    pred_reshaped =  np.transpose(pred,(0,2,1,3)).reshape(num_batches*batch_size,num_timepoints,num_classes)\n",
    "\n",
    "    #get entropies for each image & each timepoint\n",
    "    entropies_pred = np.ones([num_images_all,num_timepoints])\n",
    "    entropies_pred[:] = np.nan\n",
    "\n",
    "    for image in range(num_images_all):\n",
    "        for tp in range(num_timepoints):\n",
    "            entropies_pred[image,tp] = entropy(pred_reshaped[image,tp])\n",
    "\n",
    "    # #for each image, determine the timepoint when entropy reaches threshold\n",
    "    rt_thresh = np.ones(num_images_all)\n",
    "    rt_thresh[:] = np.nan\n",
    "    for image in range(num_images_all):\n",
    "        for tp in range(num_timepoints):\n",
    "            if entropies_pred[image,tp] <= entropy_thresh:\n",
    "                rt_thresh[image]=tp\n",
    "                break          \n",
    "\n",
    "    #if it never reaches the threshold (nan in the array), replace by 8\n",
    "    rt_thresh[np.isnan(rt_thresh)] = 8\n",
    "   \n",
    "    return rt_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick an entropy threshold that correlates the most with the EEG RTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "#load RTs\n",
    "rts_eeg_dict = sio.loadmat(os.path.join(main_path+'/rcnn-sat/','RT_all_subjects_5_35_categorization.mat'))\n",
    "rts_eeg = rts_eeg_dict.get('RTs')\n",
    "\n",
    "#define some variables\n",
    "num_subjects = rts_eeg.shape[0]\n",
    "entropies = np.arange(0.01,0.1,0.01)\n",
    "best_entropy = np.ones([num_subjects])\n",
    "best_entropy[:] = np.nan\n",
    "correlation_test = np.ones([num_subjects,3]) #all,artificial,natural\n",
    "correlation_test[:] = np.nan\n",
    "num_scenes = len(test_images_paths)\n",
    "\n",
    "#get RNN RTs for every entropy threshold and correlate with humans\n",
    "rts_rnn = np.ones([len(entropies),len(test_images_paths)])\n",
    "rts_rnn[:] = np.nan\n",
    "for idx,e in enumerate(entropies):\n",
    "    rts_rnn[idx,:] = get_RTs(test_images_paths,20,e)\n",
    "    \n",
    "#for each fold, fit the entropy threshold on 29 subjects\n",
    "for s in range(num_subjects): \n",
    "    artificial_idx = np.arange(30)\n",
    "    natural_idx = np.arange(30,60)\n",
    "\n",
    "    test_sub = rts_eeg[s,:]\n",
    "    fit_sub = np.nanmean(rts_eeg[np.arange(num_subjects)!=s,:],0)\n",
    "    correlation_fit = np.ones([len(entropies),2])\n",
    "    correlation_fit[:] = np.nan\n",
    "    corr_diff = np.ones([len(entropies)])\n",
    "    corr_diff[:] = np.nan\n",
    "    \n",
    "    for idx,e in enumerate(entropies):\n",
    "        correlation_fit[idx,0] = stats.pearsonr(np.squeeze(rts_rnn[idx,artificial_idx]),fit_sub[artificial_idx])[0] #artificial\n",
    "        correlation_fit[idx,1] = stats.pearsonr(np.squeeze(rts_rnn[idx,natural_idx]),fit_sub[natural_idx])[0] #natural\n",
    "        corr_diff[idx] = np.abs(correlation_fit[idx,0]-correlation_fit[idx,1])\n",
    "        \n",
    "    #select the entropy with highest correlation but lowest art/nat RNN-human difference   \n",
    "    best_entropy[s] = round(entropies[np.argmin(corr_diff)],2)\n",
    "    print(correlation_fit)\n",
    "    print(corr_diff)\n",
    "    \n",
    "    #remove scene if there's no RT for it \n",
    "    selected_rnn_rts = rts_rnn[np.argmin(corr_diff),:]\n",
    "    if np.argwhere(np.isnan(test_sub)).size:\n",
    "        print(s)\n",
    "        removed_scene = np.argwhere(np.isnan(test_sub))[0][0]\n",
    "        if removed_scene in natural_idx:\n",
    "            natural_idx = np.delete(natural_idx,removed_scene-30)\n",
    "        elif removed_scene in artificial_idx:\n",
    "            artificial_idx = np.delete(artificial_idx,removed_scene)\n",
    "\n",
    "    #correlate with leftout subject        \n",
    "    correlation_test[s,0] = stats.pearsonr(selected_rnn_rts[np.concatenate((artificial_idx,natural_idx))],\\\n",
    "                                           test_sub[np.concatenate((artificial_idx,natural_idx))])[0]        \n",
    "    correlation_test[s,1] = stats.pearsonr(selected_rnn_rts[artificial_idx],test_sub[artificial_idx])[0]\n",
    "    correlation_test[s,2] = stats.pearsonr(selected_rnn_rts[natural_idx],test_sub[natural_idx])[0]\n",
    "    \n",
    "print(best_entropy)\n",
    "print(correlation_test)\n",
    "RT_entropy = stats.mode(best_entropy)[0][0]\n",
    "RT_RNN_final = rts_rnn[np.argwhere(entropies==RT_entropy)[0][0],:]\n",
    "\n",
    "corr_path = os.path.join(main_path+'/rcnn-sat/'+model_name,'correlation_RT_human_RNN_cross-validated')\n",
    "np.save(corr_path,correlation_test)\n",
    "rt_path = os.path.join(main_path+'/rcnn-sat/'+model_name,'RNN_RTs_entropy_threshold_{}'.format(RT_entropy))\n",
    "np.save(rt_path,RT_RNN_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from scipy import stats\n",
    "\n",
    "# #load RTs\n",
    "# rts_eeg_dict = sio.loadmat(os.path.join(main_path+'/rcnn-sat/','RT_all_subjects_5_35_categorization.mat'))\n",
    "# rts_eeg = rts_eeg_dict.get('RTs')\n",
    "\n",
    "# #fit the entropy threshold on 29 subjects, use remaining one for further analyses\n",
    "# test_sub = rts_eeg[-1,:]\n",
    "# fit_sub_artificial = np.nanmedian(rts_eeg[0:-1,0:30],0)\n",
    "# fit_sub_natural = np.nanmedian(rts_eeg[0:-1,30:],0)\n",
    "\n",
    "# #calculate RNN-human correlations for each entropy threshold\n",
    "# entropies = np.arange(0.01,0.2,0.01)\n",
    "# correlations_art = np.ones([len(entropies)])\n",
    "# correlations_nat = np.ones([len(entropies)])\n",
    "# corr_diff = np.ones([len(entropies)])\n",
    "\n",
    "# correlations_art[:] = np.nan\n",
    "# correlations_art[:] = np.nan\n",
    "\n",
    "# rts_e = np.ones([len(entropies),len(test_images_paths)])\n",
    "# rts_e[:] = np.nan\n",
    "\n",
    "# for idx,e in enumerate(entropies):\n",
    "#     rts_e[idx,:] = get_RTs(test_images_paths,20,e)\n",
    "#     correlations_art[idx] = stats.spearmanr(np.squeeze(rts_e[idx,0:30]),fit_sub_artificial)[0]\n",
    "#     correlations_nat[idx] = stats.spearmanr(np.squeeze(rts_e[idx,30:]),fit_sub_natural)[0]\n",
    "#     corr_diff[idx] = np.abs(correlations_art[idx]-correlations_nat[idx])\n",
    "    \n",
    "# #get entropy threshold with smallest difference of RNN-human correlations for artificial and natural scenes\n",
    "# print(correlations_art)\n",
    "# print(correlations_nat)\n",
    "# best_entropy = round(entropies[np.argmin(corr_diff)],2)\n",
    "# rt_path = os.path.join(main_path+'/rcnn-sat/'+model_name,'leave_oo_cross-validated_reaction_time_entropy_th_{}'.format(best_entropy))\n",
    "# np.save(rt_path,np.squeeze(rts_e[np.argmax(correlations),:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## old ##\n",
    "\n",
    "# # from scipy import stats\n",
    "\n",
    "# rts_eeg_dict = sio.loadmat(os.path.join(main_path+'/rcnn-sat/','RT_all_subjects_5_35_categorization.mat'))\n",
    "# rts_eeg = rts_eeg_dict.get('RTs')\n",
    "# num_subjects = rts_eeg.shape[0]\n",
    "# np.random.shuffle(rts_eeg) #shuffle subjects order\n",
    "# # np.save(os.path.join(main_path+'/rcnn-sat/','RTs_shuffled_across_subjects'),rts_eeg)\n",
    "# # half_subjects = np.nanmedian(rts_eeg[0:int(num_subjects/2),:],0)\n",
    "# test_sub = rts_eeg[-1,:]\n",
    "# fit_sub = np.nanmedian(rts_eeg[0:-1,:],0)\n",
    "# entropies = np.arange(0.01,0.1,0.01)\n",
    "# correlations = np.ones([len(entropies)])\n",
    "# correlations[:] = np.nan\n",
    "# rts_e = np.ones([len(entropies),len(test_images_paths)])\n",
    "# rts_e[:] = np.nan\n",
    "\n",
    "# for idx,e in enumerate(entropies):\n",
    "#     rts_e[idx,:] = get_RTs(test_images_paths,20,e)\n",
    "#     correlations[idx] = stats.spearmanr(np.squeeze(rts_e[idx,:]),fit_sub)[0]\n",
    "    \n",
    "# best_entropy = round(entropies[np.argmax(correlations)],2)\n",
    "# rt_path = os.path.join(main_path+'/rcnn-sat/'+model_name,'cross-validated_reaction_time_entropy_th_{}'.format(best_entropy))\n",
    "# # np.save(rt_path,np.squeeze(rts_e[np.argmax(correlations),:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
