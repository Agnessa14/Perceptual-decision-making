{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from rcnn_sat import preprocess_image\n",
    "import os\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.decomposition import PCA as RandomizedPCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from scipy.stats import entropy\n",
    "from scipy import stats\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore fine-tuned model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'b_d' #b, bl, b_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Input((128, 128, 3))\n",
    "if model_type=='b':\n",
    "    from rcnn_sat import b_net\n",
    "    model = b_net(input_layer, classes_scenes=2)\n",
    "elif model_type=='b_d':\n",
    "    from rcnn_sat import b_d_net\n",
    "    model = b_d_net(input_layer, classes_scenes=2)\n",
    "elif model_type=='bl':\n",
    "    from rcnn_sat import bl_net\n",
    "    model = bl_net(input_layer, classes_scenes=2, cumulative_readout=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='{}_net_scenes'.format(model_type)\n",
    "model.load_weights(os.path.join(model_name+'_weights.h5')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '/home/agnek95/SMST/PDM_PILOT_2/ANALYSIS_Full_experiment/DNN/'\n",
    "data_path = '/scratch/agnek95/PDM/places365/val_large'\n",
    "#IDs of the scenes of interest\n",
    "with open(os.path.join(main_path,'scenes_eeg_ordered.json')) as json_file:\n",
    "    scenes_60 = json.load(json_file)      \n",
    "         \n",
    "selected_scenes = list(scenes_60.keys())\n",
    "test_images_paths = [None]*len(selected_scenes)\n",
    "for index,file in enumerate(selected_scenes):\n",
    "    test_images_paths[index] = os.path.join(data_path,selected_scenes[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get activations from all layers & all timepoints (testing data = 60 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations_rnn(img_paths,batch_size,num_layers):\n",
    "\n",
    "    ## preallocate\n",
    "    num_images_all = len(img_paths)\n",
    "    num_batches = int(num_images_all / batch_size)\n",
    "    num_timepoints = 8\n",
    "\n",
    "    ## loop over all layers and timepoints\n",
    "    for layer_idx in range(num_layers):\n",
    "        for timepoint in range(num_timepoints):\n",
    "            activ = []\n",
    "            layer_time =  'ReLU_Layer_{}_Time_{}'.format(layer_idx,timepoint)\n",
    "            get_layer_activation = tf.keras.backend.function(\n",
    "            [model.input],\n",
    "            [model.get_layer(layer_time).output])\n",
    "            for batch, img_idx in enumerate(range(0, num_images_all, batch_size)):\n",
    "                print('Getting activations for model', model_name,', layer',layer_idx,', timepoint',timepoint,', batch',batch)\n",
    "                batch_paths = img_paths[img_idx:img_idx + batch_size] \n",
    "                batch_images = np.zeros((batch_size,128,128,3)) \n",
    "                #preprocessing images\n",
    "                for i, image_path in enumerate(batch_paths):\n",
    "                    image = load_img(image_path, target_size=(128, 128)) \n",
    "                    image = img_to_array(image)\n",
    "                    image = np.uint8(image)\n",
    "                    image = preprocess_image(image)\n",
    "                    batch_images[i,:,:,:] = image\n",
    "\n",
    "                activ.append(list(np.array(get_layer_activation(batch_images)).squeeze()))  \n",
    "            \n",
    "#           flatten the vector of activations from all batches into (num-all-images x num-all-features)    \n",
    "            flattened_activ = np.array(activ).reshape(num_images_all,-1)\n",
    "\n",
    "            #replace zeros\n",
    "            flattened_activ[flattened_activ==0]=0.000001\n",
    "            \n",
    "            #z-score across images\n",
    "            zscored_activ = stats.zscore(flattened_activ,axis=0) #normalize over images\n",
    "            print(np.mean(zscored_activ[:,0])) #get the mean of feature 1 values for all imgs - should be 0\n",
    "            path = os.path.join('/scratch/agnek95/PDM/DATA/RNN_ACTIVATIONS/activations','{}_activations'.format(layer_time))\n",
    "            np.save(path,zscored_activ)\n",
    "            print('Saved zscored activations')\n",
    "\n",
    "            del flattened_activ\n",
    "            del activ\n",
    "            del zscored_activ\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations_ff(img_paths,batch_size,num_layers,train_or_test):\n",
    "## Parameters:\n",
    "# -img_paths\n",
    "# -batch_size\n",
    "# -num_layers\n",
    "\n",
    "    ## preallocate\n",
    "    num_images_all = len(img_paths)\n",
    "    print(num_images_all)\n",
    "    num_batches = int(num_images_all / batch_size)\n",
    "    if model_type=='b':\n",
    "        range_layers=range(num_layers)\n",
    "    elif model_type=='b_d':\n",
    "        range_layers=range(1,num_layers,2)\n",
    "\n",
    "    ## loop over all layers and timepoints\n",
    "    for layer_idx in range_layers:\n",
    "        activ = []\n",
    "        layer =  'ReLU_Layer_{}'.format(layer_idx)\n",
    "        get_layer_activation = tf.keras.backend.function(\n",
    "        [model.input],\n",
    "        [model.get_layer(layer).output])\n",
    "        for batch, img_idx in enumerate(range(0, num_images_all, batch_size)):\n",
    "            print('Getting', train_or_test, 'activations for model', model_name,', layer',layer_idx,', batch',batch)\n",
    "            batch_paths = img_paths[img_idx:img_idx + batch_size] \n",
    "            batch_images = np.zeros((batch_size,128,128,3)) \n",
    "            #preprocessing images\n",
    "            for i, image_path in enumerate(batch_paths):\n",
    "                image = load_img(image_path, target_size=(128, 128)) \n",
    "                image = img_to_array(image)\n",
    "                image = np.uint8(image)\n",
    "                image = preprocess_image(image)\n",
    "                batch_images[i,:,:,:] = image\n",
    "\n",
    "            activ.append(list(np.array(get_layer_activation(batch_images)).squeeze()))  \n",
    "\n",
    "        print('Flattening') #flatten the vector of activations from all batches into (num-all-images x num-all-features)    \n",
    "        flattened_activ = np.array(activ).reshape(num_images_all,-1)\n",
    "\n",
    "        #replace zeros\n",
    "        flattened_activ[flattened_activ==0]=0.000001\n",
    "        \n",
    "        #z-score across images\n",
    "        print('Z-scoring')\n",
    "        zscored_activ = stats.zscore(flattened_activ,axis=0) #normalize over images\n",
    "                   \n",
    "        \n",
    "        print(np.mean(zscored_activ[:,0])) #get the mean of feature 1 values for all imgs - should be 0\n",
    "        if model_type=='b_d':\n",
    "            path_name='B_D'\n",
    "        elif model_type=='b':\n",
    "            path_name='B'\n",
    "\n",
    "        print('Saving')\n",
    "        if train_or_test=='train':\n",
    "            path = os.path.join('/scratch/agnek95/PDM/DATA/{}_ACTIVATIONS/train_activations'.format(path_name),'{}_activations'.format(layer))\n",
    "        elif train_or_test=='test':\n",
    "            path = os.path.join('/scratch/agnek95/PDM/DATA/{}_ACTIVATIONS/activations'.format(path_name),'{}_activations'.format(layer))\n",
    "        np.save(path,zscored_activ)\n",
    "        print('Saved zscored activations')\n",
    "\n",
    "        del flattened_activ\n",
    "        del activ\n",
    "        del zscored_activ\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if needed, include in the get_activations function\n",
    "def run_pca(activations,num_components):\n",
    "    np.random.seed(0)   \n",
    "    scaler = MinMaxScaler(feature_range=[0 ,1]) #normalize\n",
    "    activs_scaled = scaler.fit_transform(activations)\n",
    "    pca = RandomizedPCA(n_components = num_components)\n",
    "    pca_results = pca.fit_transform(activs_scaled)\n",
    "    \n",
    "    return pca_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define number of layers\n",
    "if model_type=='b' or model_type=='bl':\n",
    "    n_layers=7\n",
    "elif model_type=='b_d':\n",
    "    n_layers=14\n",
    "\n",
    "#run appropriate function to get activations\n",
    "\n",
    "if model_type=='b' or model_type=='b_d': #feedforward\n",
    "    # get_activations_ff(train_images_paths_selected,60,n_layers,'train') #train activations\n",
    "    get_activations_ff(test_images_paths,60,n_layers,'test')  #test activations\n",
    "elif model_type=='b_l':\n",
    "    get_activations_rnn(test_images_paths,60,n_layers,'test') #test activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get RTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RTs(test_images_path,batch_size,entropy_thresh):\n",
    "    num_images_all = len(test_images_path)\n",
    "    num_batches = int(num_images_all / batch_size)\n",
    "    num_timepoints = 8\n",
    "    num_classes = 2\n",
    "    all_batches_activ = np.ones([num_batches, batch_size, 128, 128, 3])\n",
    "    all_batches_activ[:] = np.nan\n",
    "    pred = np.ones([num_batches,num_timepoints,batch_size,num_classes])\n",
    "    pred[:] = np.nan\n",
    "\n",
    "    for batch, img_idx in enumerate(range(0, num_images_all, batch_size)):\n",
    "        batch_paths = test_images_path[img_idx:img_idx + batch_size] \n",
    "        batch_images = np.zeros((batch_size,128,128,3)) \n",
    "        for i, image_path in enumerate(batch_paths):\n",
    "            image = load_img(image_path, target_size=(128, 128)) \n",
    "            image = img_to_array(image)\n",
    "            image = np.uint8(image)\n",
    "            image = preprocess_image(image)\n",
    "            batch_images[i,:,:,:] = image\n",
    "\n",
    "        #predictions\n",
    "        pred[batch,:,:,:] = model(batch_images) #shape: num_timepoints x batch_size x classes\n",
    "\n",
    "    #reshape: all images from all batches in one dimension\n",
    "    pred_reshaped =  np.transpose(pred,(0,2,1,3)).reshape(num_batches*batch_size,num_timepoints,num_classes)\n",
    "\n",
    "    #get entropies for each image & each timepoint\n",
    "    entropies_pred = np.ones([num_images_all,num_timepoints])\n",
    "    entropies_pred[:] = np.nan\n",
    "\n",
    "    for image in range(num_images_all):\n",
    "        for tp in range(num_timepoints):\n",
    "            entropies_pred[image,tp] = entropy(pred_reshaped[image,tp])\n",
    "\n",
    "    # #for each image, determine the timepoint when entropy reaches threshold\n",
    "    rt_thresh = np.ones(num_images_all)\n",
    "    rt_thresh[:] = np.nan\n",
    "    for image in range(num_images_all):\n",
    "        for tp in range(num_timepoints):\n",
    "            if entropies_pred[image,tp] <= entropy_thresh:\n",
    "                rt_thresh[image]=tp\n",
    "                break          \n",
    "\n",
    "    #if it never reaches the threshold (nan in the array), replace by 8\n",
    "    rt_thresh[np.isnan(rt_thresh)] = 8\n",
    "   \n",
    "    return rt_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick an entropy threshold that correlates the most with the EEG RTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "#load RTs\n",
    "rts_eeg_dict = sio.loadmat(os.path.join(main_path,'RT_all_subjects_5_35_categorization.mat'))\n",
    "rts_eeg = rts_eeg_dict.get('RTs')\n",
    "\n",
    "#define some variables\n",
    "num_subjects = rts_eeg.shape[0]\n",
    "entropies = np.arange(0.01,0.1,0.01)\n",
    "best_entropy = np.ones([num_subjects])\n",
    "best_entropy[:] = np.nan\n",
    "correlation_test = np.ones([num_subjects,3]) #all,artificial,natural\n",
    "correlation_test[:] = np.nan\n",
    "num_scenes = len(test_images_paths)\n",
    "\n",
    "#get RNN RTs for every entropy threshold and correlate with humans\n",
    "rts_rnn = np.ones([len(entropies),len(test_images_paths)])\n",
    "rts_rnn[:] = np.nan\n",
    "for idx,e in enumerate(entropies):\n",
    "    rts_rnn[idx,:] = get_RTs(test_images_paths,20,e)\n",
    "    \n",
    "#for each fold, fit the entropy threshold on 29 subjects\n",
    "for s in range(num_subjects): \n",
    "    artificial_idx = np.arange(30)\n",
    "    natural_idx = np.arange(30,60)\n",
    "\n",
    "    test_sub = rts_eeg[s,:]\n",
    "    fit_sub = np.nanmean(rts_eeg[np.arange(num_subjects)!=s,:],0)\n",
    "    correlation_fit = np.ones([len(entropies),2])\n",
    "    correlation_fit[:] = np.nan\n",
    "    corr_diff = np.ones([len(entropies)])\n",
    "    corr_diff[:] = np.nan\n",
    "    \n",
    "    for idx,e in enumerate(entropies):\n",
    "        correlation_fit[idx,0] = stats.pearsonr(np.squeeze(rts_rnn[idx,artificial_idx]),fit_sub[artificial_idx])[0] #artificial\n",
    "        correlation_fit[idx,1] = stats.pearsonr(np.squeeze(rts_rnn[idx,natural_idx]),fit_sub[natural_idx])[0] #natural\n",
    "        corr_diff[idx] = np.abs(correlation_fit[idx,0]-correlation_fit[idx,1])\n",
    "        \n",
    "    #select the entropy with highest correlation but lowest art/nat RNN-human difference   \n",
    "    best_entropy[s] = round(entropies[np.argmin(corr_diff)],2)\n",
    "    print(correlation_fit)\n",
    "    print(corr_diff)\n",
    "    \n",
    "    #remove scene if there's no RT for it \n",
    "    selected_rnn_rts = rts_rnn[np.argmin(corr_diff),:]\n",
    "    if np.argwhere(np.isnan(test_sub)).size:\n",
    "        print(s)\n",
    "        removed_scene = np.argwhere(np.isnan(test_sub))[0][0]\n",
    "        if removed_scene in natural_idx:\n",
    "            natural_idx = np.delete(natural_idx,removed_scene-30)\n",
    "        elif removed_scene in artificial_idx:\n",
    "            artificial_idx = np.delete(artificial_idx,removed_scene)\n",
    "\n",
    "    #correlate with leftout subject        \n",
    "    correlation_test[s,0] = stats.pearsonr(selected_rnn_rts[np.concatenate((artificial_idx,natural_idx))],\\\n",
    "                                           test_sub[np.concatenate((artificial_idx,natural_idx))])[0]        \n",
    "    correlation_test[s,1] = stats.pearsonr(selected_rnn_rts[artificial_idx],test_sub[artificial_idx])[0]\n",
    "    correlation_test[s,2] = stats.pearsonr(selected_rnn_rts[natural_idx],test_sub[natural_idx])[0]\n",
    "    \n",
    "print(best_entropy)\n",
    "print(correlation_test)\n",
    "RT_entropy = stats.mode(best_entropy)[0][0]\n",
    "RT_RNN_final = rts_rnn[np.argwhere(entropies==RT_entropy)[0][0],:]\n",
    "\n",
    "corr_path = os.path.join(main_path,'correlation_RT_human_RNN_cross-validated')\n",
    "np.save(corr_path,correlation_test)\n",
    "rt_path = os.path.join(main_path,'RNN_RTs_entropy_threshold_{}'.format(RT_entropy))\n",
    "np.save(rt_path,RT_RNN_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNN_env_kernel",
   "language": "python",
   "name": "dnn_env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
