{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from rcnn_sat import preprocess_image\n",
    "import os\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.decomposition import PCA as RandomizedPCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import entropy\n",
    "from scipy import stats\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning: train the classification layer on scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'b' #b, bl, b_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Input((128, 128, 3))\n",
    "if model_type=='b':\n",
    "    from rcnn_sat import b_net\n",
    "    model = b_net(input_layer, classes_scenes=2)\n",
    "elif model_type=='b_d':\n",
    "    from rcnn_sat import b_d_net\n",
    "    model = b_d_net(input_layer, classes_scenes=2)\n",
    "elif model_type=='bl':\n",
    "    from rcnn_sat import bl_net\n",
    "    model = bl_net(input_layer, classes_scenes=2, cumulative_readout=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the weights of the frozen layers into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_name='{}_ecoset.h5'.format(model_type)\n",
    "#download from OSF if not done already\n",
    "if not os.path.isfile(weights_name): \n",
    "    _, msg = urllib.request.urlretrieve(\n",
    "        'https://osf.io/9td5p/download', '{}_ecoset.h5'.format(model_type))\n",
    "    print(msg)\n",
    "model.load_weights(weights_name,by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define trainable layers (for fine-tuning: all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and preprocess training & validation data\n",
    "\n",
    "#training\n",
    "with open('2400_selected_scenes_places365_train_standard.json') as json_file:\n",
    "    subset_scenes_dict_train = json.load(json_file)  \n",
    "train_set_path = '/scratch/agnek95/PDM/places_365_256_train_val/data_256/'\n",
    "train_image_paths = list(subset_scenes_dict_train.keys())\n",
    "train_imgs_prep = np.ones([len(train_image_paths),128,128,3])\n",
    "train_imgs_prep[:] = np.nan\n",
    "for idx,image_path in enumerate(train_image_paths):\n",
    "        image = load_img(train_set_path+image_path, target_size=(128, 128)) \n",
    "        image = img_to_array(image)\n",
    "        image = np.uint8(image)\n",
    "        image = preprocess_image(image)\n",
    "        train_imgs_prep[idx,:,:,:] = image\n",
    "        \n",
    "train_images_paths = [train_set_path+image_path for image_path in train_image_paths]\n",
    "\n",
    "#validation    \n",
    "with open('1200_selected_scenes_places365_val_standard.json') as json_file:\n",
    "    subset_scenes_dict_val = json.load(json_file)  \n",
    "val_set_path = '/scratch/agnek95/PDM/places_365_256_train_val/val_256/' \n",
    "val_image_paths = list(subset_scenes_dict_val.keys())\n",
    "val_imgs_prep = np.ones([len(val_image_paths),128,128,3])\n",
    "val_imgs_prep[:] = np.nan\n",
    "for idx,image_path in enumerate(val_image_paths):\n",
    "        image = load_img(val_set_path+image_path, target_size=(128, 128)) \n",
    "        image = img_to_array(image)\n",
    "        image = np.uint8(image)\n",
    "        image = preprocess_image(image)\n",
    "        val_imgs_prep[idx,:,:,:] = image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels: man-made: 0, natural: 1\n",
    "y_train = np.array([label for label in range(2) for reps in range(int(train_imgs_prep.shape[0]/2))]) \n",
    "y_val = np.array([label for label in range(2) for reps in range(int(val_imgs_prep.shape[0]/2))])\n",
    "\n",
    "x_train =  train_imgs_prep\n",
    "x_val = val_imgs_prep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001 #in case you want to use optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "if model.name == 'bl_net_edl':\n",
    "\n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(x_val.shape)\n",
    "    print(y_val.shape)\n",
    "        \n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    shuffle=True,\n",
    "    batch_size=10,\n",
    "    epochs=20, #5?\n",
    "    validation_data=(x_val, y_val),\n",
    ")\n",
    "\n",
    "# y_pred = model.predict(x_val, batch_size = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_val, batch_size = 10)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='{}_net_scenes'.format(model_type)\n",
    "model.save_weights(model_name+'_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you want to get the RTs rightaway, follow the next steps, otherwise use the separate script (rnn_dth_collect_activations.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RTs(test_images_path,batch_size,entropy_thresh):\n",
    "    num_images_all = len(test_images_path)\n",
    "    num_batches = int(num_images_all / batch_size)\n",
    "    num_timepoints = 8\n",
    "    num_classes = 2\n",
    "    all_batches_activ = np.ones([num_batches, batch_size, 128, 128, 3])\n",
    "    all_batches_activ[:] = np.nan\n",
    "    pred = np.ones([num_batches,num_timepoints,batch_size,num_classes])\n",
    "    pred[:] = np.nan\n",
    "\n",
    "    for batch, img_idx in enumerate(range(0, num_images_all, batch_size)):\n",
    "        batch_paths = test_images_path[img_idx:img_idx + batch_size] \n",
    "        batch_images = np.zeros((batch_size,128,128,3)) \n",
    "        for i, image_path in enumerate(batch_paths):\n",
    "            image = load_img(image_path, target_size=(128, 128)) \n",
    "            image = img_to_array(image)\n",
    "            image = np.uint8(image)\n",
    "            image = preprocess_image(image)\n",
    "            batch_images[i,:,:,:] = image\n",
    "\n",
    "        #predictions\n",
    "        pred[batch,:,:,:] = model(batch_images) #shape: num_timepoints x batch_size x classes\n",
    "\n",
    "    #reshape: all images from all batches in one dimension\n",
    "    pred_reshaped =  np.transpose(pred,(0,2,1,3)).reshape(num_batches*batch_size,num_timepoints,num_classes)\n",
    "\n",
    "    #get entropies for each image & each timepoint\n",
    "    entropies_pred = np.ones([num_images_all,num_timepoints])\n",
    "    entropies_pred[:] = np.nan\n",
    "\n",
    "    for image in range(num_images_all):\n",
    "        for tp in range(num_timepoints):\n",
    "            entropies_pred[image,tp] = entropy(pred_reshaped[image,tp])\n",
    "\n",
    "    # #for each image, determine the timepoint when entropy reaches threshold\n",
    "    rt_thresh = np.ones(num_images_all)\n",
    "    rt_thresh[:] = np.nan\n",
    "    for image in range(num_images_all):\n",
    "        for tp in range(num_timepoints):\n",
    "            if entropies_pred[image,tp] <= entropy_thresh:\n",
    "                rt_thresh[image]=tp\n",
    "                break          \n",
    "\n",
    "    #if it never reaches the threshold (nan in the array), replace by 8\n",
    "    rt_thresh[np.isnan(rt_thresh)] = 8\n",
    "   \n",
    "    return rt_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "#load RTs\n",
    "rts_eeg_dict = sio.loadmat('RT_all_subjects_5_35_categorization.mat')\n",
    "rts_eeg = rts_eeg_dict.get('RTs')\n",
    "\n",
    "#define some variables\n",
    "num_subjects = rts_eeg.shape[0]\n",
    "entropies = np.arange(0.01,0.1,0.01)\n",
    "best_entropy = np.ones([num_subjects])\n",
    "best_entropy[:] = np.nan\n",
    "correlation_test = np.ones([num_subjects,3]) #all,artificial,natural\n",
    "correlation_test[:] = np.nan\n",
    "num_scenes = len(test_images_paths)\n",
    "\n",
    "#get RNN RTs for every entropy threshold and correlate with humans\n",
    "rts_rnn = np.ones([len(entropies),len(test_images_paths)])\n",
    "rts_rnn[:] = np.nan\n",
    "for idx,e in enumerate(entropies):\n",
    "    rts_rnn[idx,:] = get_RTs(test_images_paths,20,e)\n",
    "    \n",
    "#for each fold, fit the entropy threshold on 29 subjects\n",
    "for s in range(num_subjects): \n",
    "    artificial_idx = np.arange(30)\n",
    "    natural_idx = np.arange(30,60)\n",
    "\n",
    "    test_sub = rts_eeg[s,:]\n",
    "    fit_sub = np.nanmean(rts_eeg[np.arange(num_subjects)!=s,:],0)\n",
    "    correlation_fit = np.ones([len(entropies),2])\n",
    "    correlation_fit[:] = np.nan\n",
    "    corr_diff = np.ones([len(entropies)])\n",
    "    corr_diff[:] = np.nan\n",
    "    \n",
    "    for idx,e in enumerate(entropies):\n",
    "        correlation_fit[idx,0] = stats.pearsonr(np.squeeze(rts_rnn[idx,artificial_idx]),fit_sub[artificial_idx])[0] #artificial\n",
    "        correlation_fit[idx,1] = stats.pearsonr(np.squeeze(rts_rnn[idx,natural_idx]),fit_sub[natural_idx])[0] #natural\n",
    "        corr_diff[idx] = np.abs(correlation_fit[idx,0]-correlation_fit[idx,1])\n",
    "        \n",
    "    #select the entropy with highest correlation but lowest art/nat RNN-human difference   \n",
    "    best_entropy[s] = round(entropies[np.argmin(corr_diff)],2)\n",
    "    print(correlation_fit)\n",
    "    print(corr_diff)\n",
    "    \n",
    "    #remove scene if there's no RT for it \n",
    "    selected_rnn_rts = rts_rnn[np.argmin(corr_diff),:]\n",
    "    if np.argwhere(np.isnan(test_sub)).size:\n",
    "        print(s)\n",
    "        removed_scene = np.argwhere(np.isnan(test_sub))[0][0]\n",
    "        if removed_scene in natural_idx:\n",
    "            natural_idx = np.delete(natural_idx,removed_scene-30)\n",
    "        elif removed_scene in artificial_idx:\n",
    "            artificial_idx = np.delete(artificial_idx,removed_scene)\n",
    "\n",
    "    #correlate with leftout subject        \n",
    "    correlation_test[s,0] = stats.pearsonr(selected_rnn_rts[np.concatenate((artificial_idx,natural_idx))],\\\n",
    "                                           test_sub[np.concatenate((artificial_idx,natural_idx))])[0]        \n",
    "    correlation_test[s,1] = stats.pearsonr(selected_rnn_rts[artificial_idx],test_sub[artificial_idx])[0]\n",
    "    correlation_test[s,2] = stats.pearsonr(selected_rnn_rts[natural_idx],test_sub[natural_idx])[0]\n",
    "    \n",
    "print(best_entropy)\n",
    "print(correlation_test)\n",
    "RT_entropy = stats.mode(best_entropy)[0][0]\n",
    "RT_RNN_final = rts_rnn[np.argwhere(entropies==RT_entropy)[0][0],:]\n",
    "\n",
    "np.save('correlation_RT_human_{}_net_cross-validated'.format(model_type),correlation_test)\n",
    "np.save('{}_net_RTs_entropy_threshold_{}'.format(model_type,RT_entropy),RT_RNN_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNN_env_kernel",
   "language": "python",
   "name": "dnn_env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
