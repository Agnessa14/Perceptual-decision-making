{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from rcnn_sat_2 import preprocess_image, bl_net_edl\n",
    "import evidential_deep_learning as edl\n",
    "\n",
    "import os\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning: train the classification layer on scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Input((128, 128, 3))\n",
    "model = bl_net(input_layer, classes_scenes=2, cumulative_readout=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the weights of the frozen layers into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, msg = urllib.request.urlretrieve(\n",
    "#     'https://osf.io/9td5p/download', 'bl_ecoset.h5')\n",
    "# print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the weights from the fine-tuned (Places) model\n",
    "# model_name = 'model_02.11_2'\n",
    "# data_names_path = '/mnt/raid/ni/agnessa/rcnn-sat/'\n",
    "\n",
    "# save_path = data_names_path+model_name\n",
    "# model.load_weights(os.path.join(save_path,model_name+'_weights.h5'),by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('bl_ecoset.h5',by_name=True) # ,by_name=True only loads the weights with the same name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define trainable layers (for fine-tuning: all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model.layers:\n",
    "#     if 'sigmoid' in layer.name or 'readout' in layer.name or 'Readout' in layer.name or 'dirichlet' in layer.name:\n",
    "#         layer.trainable = True\n",
    "#     else:\n",
    "#         layer.trainable = False\n",
    "               \n",
    "# for layer in model.layers:\n",
    "#     print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) create json files of names+labels of selected training/validation data (5 per class for training, 2 per class for val)\n",
    "#   -- not all classes shown in txt file... don't need all i think. so just pick 5 from 200 classes\n",
    "#2) upload the training data onto eltanin\n",
    "#3) run through "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and preprocess training & validation data\n",
    "data_names_path = '/mnt/raid/ni/agnessa/rcnn-sat/'\n",
    "#training\n",
    "with open(os.path.join(data_names_path,'2400_selected_scenes_places365_train_standard.json')) as json_file:\n",
    "    subset_scenes_dict_train = json.load(json_file)  \n",
    "train_set_path = '/mnt/raid/data/agnessa/data_256'\n",
    "train_image_paths = list(subset_scenes_dict_train.keys())\n",
    "train_imgs_prep = np.ones([len(train_image_paths),128,128,3])\n",
    "train_imgs_prep[:] = np.nan\n",
    "for idx,image_path in enumerate(train_image_paths):\n",
    "        image = load_img(train_set_path+image_path, target_size=(128, 128)) \n",
    "        image = img_to_array(image)\n",
    "        image = np.uint8(image)\n",
    "        image = preprocess_image(image)\n",
    "        train_imgs_prep[idx,:,:,:] = image\n",
    "        \n",
    "train_images_paths = [train_set_path+image_path for image_path in train_image_paths]\n",
    "\n",
    "#validation    \n",
    "with open(os.path.join(data_names_path,'1200_selected_scenes_places365_val_standard.json')) as json_file:\n",
    "    subset_scenes_dict_val = json.load(json_file)  \n",
    "val_set_path = '/mnt/raid/data/agnessa/val_256/' \n",
    "val_image_paths = list(subset_scenes_dict_val.keys())\n",
    "val_imgs_prep = np.ones([len(val_image_paths),128,128,3])\n",
    "val_imgs_prep[:] = np.nan\n",
    "for idx,image_path in enumerate(val_image_paths):\n",
    "        image = load_img(val_set_path+image_path, target_size=(128, 128)) \n",
    "        image = img_to_array(image)\n",
    "        image = np.uint8(image)\n",
    "        image = preprocess_image(image)\n",
    "        val_imgs_prep[idx,:,:,:] = image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels: artificial: 0, natural: 1\n",
    "y_train = np.array([label for label in range(2) for reps in range(int(train_imgs_prep.shape[0]/2))]) \n",
    "y_val = np.array([label for label in range(2) for reps in range(int(val_imgs_prep.shape[0]/2))])\n",
    "\n",
    "x_train =  train_imgs_prep\n",
    "x_val = val_imgs_prep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001 #in case you want to use optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "#               loss=edl.losses.Dirichlet_SOS,\n",
    "#               loss = edl.losses.Sigmoid_CE,\n",
    "#                 loss=edl.losses.MSE,\n",
    "\n",
    "              metrics=['accuracy'])\n",
    "if model.name == 'bl_net_edl':\n",
    "\n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(x_val.shape)\n",
    "    print(y_val.shape)\n",
    "        \n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    shuffle=True,\n",
    "    batch_size=10,\n",
    "    epochs=20, #5?\n",
    "    validation_data=(x_val, y_val),\n",
    ")\n",
    "\n",
    "# y_pred = model.predict(x_val, batch_size = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_val, batch_size = 10)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_02.11_2'\n",
    "save_path = data_names_path+model_name\n",
    "model.save_weights(os.path.join(save_path,model_name+'_weights.h5'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you want to get the RTs rightaway, follow the next steps, otherwise use the separate script (rnn_dth_collect_activations.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RTs(test_images_path,batch_size,entropy_thresh):\n",
    "    num_images_all = len(test_images_path)\n",
    "    num_batches = int(num_images_all / batch_size)\n",
    "    num_timepoints = 8\n",
    "    num_classes = 2\n",
    "    all_batches_activ = np.ones([num_batches, batch_size, 128, 128, 3])\n",
    "    all_batches_activ[:] = np.nan\n",
    "    pred = np.ones([num_batches,num_timepoints,batch_size,num_classes])\n",
    "    pred[:] = np.nan\n",
    "\n",
    "    for batch, img_idx in enumerate(range(0, num_images_all, batch_size)):\n",
    "        batch_paths = test_images_path[img_idx:img_idx + batch_size] \n",
    "        batch_images = np.zeros((batch_size,128,128,3)) \n",
    "        for i, image_path in enumerate(batch_paths):\n",
    "            image = load_img(image_path, target_size=(128, 128)) \n",
    "            image = img_to_array(image)\n",
    "            image = np.uint8(image)\n",
    "            image = preprocess_image(image)\n",
    "            batch_images[i,:,:,:] = image\n",
    "\n",
    "        #predictions\n",
    "        pred[batch,:,:,:] = model(batch_images) #shape: num_timepoints x batch_size x classes\n",
    "\n",
    "    #reshape: all images from all batches in one dimension\n",
    "    pred_reshaped =  np.transpose(pred,(0,2,1,3)).reshape(num_batches*batch_size,num_timepoints,num_classes)\n",
    "\n",
    "    #get entropies for each image & each timepoint\n",
    "    entropies_pred = np.ones([num_images_all,num_timepoints])\n",
    "    entropies_pred[:] = np.nan\n",
    "\n",
    "    for image in range(num_images_all):\n",
    "        for tp in range(num_timepoints):\n",
    "            entropies_pred[image,tp] = entropy(pred_reshaped[image,tp])\n",
    "\n",
    "    # #for each image, determine the timepoint when entropy reaches threshold\n",
    "    rt_thresh = np.ones(num_images_all)\n",
    "    rt_thresh[:] = np.nan\n",
    "    for image in range(num_images_all):\n",
    "        for tp in range(num_timepoints):\n",
    "            if entropies_pred[image,tp] <= entropy_thresh:\n",
    "                rt_thresh[image]=tp\n",
    "                break          \n",
    "\n",
    "    #if it never reaches the threshold (nan in the array), replace by 8\n",
    "    rt_thresh[np.isnan(rt_thresh)] = 8\n",
    "   \n",
    "    return rt_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "#load RTs\n",
    "rts_eeg_dict = sio.loadmat(os.path.join(main_path+'/rcnn-sat/','RT_all_subjects_5_35_categorization.mat'))\n",
    "rts_eeg = rts_eeg_dict.get('RTs')\n",
    "\n",
    "#define some variables\n",
    "num_subjects = rts_eeg.shape[0]\n",
    "entropies = np.arange(0.01,0.1,0.01)\n",
    "best_entropy = np.ones([num_subjects])\n",
    "best_entropy[:] = np.nan\n",
    "correlation_test = np.ones([num_subjects,3]) #all,artificial,natural\n",
    "correlation_test[:] = np.nan\n",
    "num_scenes = len(test_images_paths)\n",
    "\n",
    "#get RNN RTs for every entropy threshold and correlate with humans\n",
    "rts_rnn = np.ones([len(entropies),len(test_images_paths)])\n",
    "rts_rnn[:] = np.nan\n",
    "for idx,e in enumerate(entropies):\n",
    "    rts_rnn[idx,:] = get_RTs(test_images_paths,20,e)\n",
    "    \n",
    "#for each fold, fit the entropy threshold on 29 subjects\n",
    "for s in range(num_subjects): \n",
    "    artificial_idx = np.arange(30)\n",
    "    natural_idx = np.arange(30,60)\n",
    "\n",
    "    test_sub = rts_eeg[s,:]\n",
    "    fit_sub = np.nanmean(rts_eeg[np.arange(num_subjects)!=s,:],0)\n",
    "    correlation_fit = np.ones([len(entropies),2])\n",
    "    correlation_fit[:] = np.nan\n",
    "    corr_diff = np.ones([len(entropies)])\n",
    "    corr_diff[:] = np.nan\n",
    "    \n",
    "    for idx,e in enumerate(entropies):\n",
    "        correlation_fit[idx,0] = stats.pearsonr(np.squeeze(rts_rnn[idx,artificial_idx]),fit_sub[artificial_idx])[0] #artificial\n",
    "        correlation_fit[idx,1] = stats.pearsonr(np.squeeze(rts_rnn[idx,natural_idx]),fit_sub[natural_idx])[0] #natural\n",
    "        corr_diff[idx] = np.abs(correlation_fit[idx,0]-correlation_fit[idx,1])\n",
    "        \n",
    "    #select the entropy with highest correlation but lowest art/nat RNN-human difference   \n",
    "    best_entropy[s] = round(entropies[np.argmin(corr_diff)],2)\n",
    "    print(correlation_fit)\n",
    "    print(corr_diff)\n",
    "    \n",
    "    #remove scene if there's no RT for it \n",
    "    selected_rnn_rts = rts_rnn[np.argmin(corr_diff),:]\n",
    "    if np.argwhere(np.isnan(test_sub)).size:\n",
    "        print(s)\n",
    "        removed_scene = np.argwhere(np.isnan(test_sub))[0][0]\n",
    "        if removed_scene in natural_idx:\n",
    "            natural_idx = np.delete(natural_idx,removed_scene-30)\n",
    "        elif removed_scene in artificial_idx:\n",
    "            artificial_idx = np.delete(artificial_idx,removed_scene)\n",
    "\n",
    "    #correlate with leftout subject        \n",
    "    correlation_test[s,0] = stats.pearsonr(selected_rnn_rts[np.concatenate((artificial_idx,natural_idx))],\\\n",
    "                                           test_sub[np.concatenate((artificial_idx,natural_idx))])[0]        \n",
    "    correlation_test[s,1] = stats.pearsonr(selected_rnn_rts[artificial_idx],test_sub[artificial_idx])[0]\n",
    "    correlation_test[s,2] = stats.pearsonr(selected_rnn_rts[natural_idx],test_sub[natural_idx])[0]\n",
    "    \n",
    "print(best_entropy)\n",
    "print(correlation_test)\n",
    "RT_entropy = stats.mode(best_entropy)[0][0]\n",
    "RT_RNN_final = rts_rnn[np.argwhere(entropies==RT_entropy)[0][0],:]\n",
    "\n",
    "corr_path = os.path.join(main_path+'/rcnn-sat/'+model_name,'correlation_RT_human_RNN_cross-validated')\n",
    "np.save(corr_path,correlation_test)\n",
    "rt_path = os.path.join(main_path+'/rcnn-sat/'+model_name,'RNN_RTs_entropy_threshold_{}'.format(RT_entropy))\n",
    "np.save(rt_path,RT_RNN_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_path = '/mnt/raid/ni/agnessa'\n",
    "# data_path = '/mnt/raid/data/agnessa/val_256'\n",
    "\n",
    "# #IDs of the scenes of interest\n",
    "# with open(os.path.join(main_path,'RSA/RSA_EEG/scenes_eeg_ordered.json')) as json_file:\n",
    "#     scenes_60 = json.load(json_file)      \n",
    "         \n",
    "# selected_scenes = list(scenes_60.keys())\n",
    "# test_images_paths = [None]*len(selected_scenes)\n",
    "# for index,file in enumerate(selected_scenes):\n",
    "#     test_images_paths[index] = os.path.join(data_path,selected_scenes[index])\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# for batch\n",
    "# for image_path in test_images_paths:\n",
    "#     image = load_img(image_path, target_size=(128, 128)) \n",
    "#     image = img_to_array(image)\n",
    "#     image = np.uint8(image)\n",
    "#     image = preprocess_image(image)\n",
    "#     batch_images[i,:,:,:] = image\n",
    "\n",
    "# #     plt.imshow(image)\n",
    "# #     plt.show()\n",
    "\n",
    "\n",
    "# num_images_test = 60\n",
    "\n",
    "\n",
    "# batch_images = np.zeros((num_images_test,128,128,3))\n",
    "# for i, image_path in enumerate(test_images_paths):\n",
    "#     image = load_img(image_path, target_size=(128, 128)) \n",
    "#     image = img_to_array(image)\n",
    "#     image = np.uint8(image)\n",
    "#     image = preprocess_image(image)\n",
    "#     batch_images[i,:,:,:] = image\n",
    "        \n",
    "# #     plt.imshow(image)\n",
    "# #     plt.show()\n",
    "    \n",
    "# pred = model.predict(batch_images)\n",
    "# labels = np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_RTs(test_images_path,batch_size,entropy_thresh):\n",
    "#     num_images_all = len(test_images_path)\n",
    "#     num_batches = int(num_images_all / batch_size)\n",
    "#     num_timepoints = 8\n",
    "#     num_classes = 2\n",
    "#     all_batches_activ = np.ones([num_batches, batch_size, 128, 128, 3])\n",
    "#     all_batches_activ[:] = np.nan\n",
    "#     pred = np.ones([num_batches,num_timepoints,batch_size,num_classes])\n",
    "#     pred[:] = np.nan\n",
    "\n",
    "#     for batch, img_idx in enumerate(range(0, num_images_all, batch_size)):\n",
    "#         batch_paths = test_images_path[img_idx:img_idx + batch_size] \n",
    "#         batch_images = np.zeros((batch_size,128,128,3)) \n",
    "#         for i, image_path in enumerate(batch_paths):\n",
    "#             image = load_img(image_path, target_size=(128, 128)) \n",
    "#             image = img_to_array(image)\n",
    "#             image = np.uint8(image)\n",
    "#             image = preprocess_image(image)\n",
    "#             batch_images[i,:,:,:] = image\n",
    "\n",
    "#         #predictions\n",
    "#         pred[batch,:,:,:] = model(batch_images) #shape: num_timepoints x batch_size x classes\n",
    "\n",
    "#     #reshape: all images from all batches in one dimension\n",
    "#     pred_reshaped =  np.transpose(pred,(0,2,1,3)).reshape(num_batches*batch_size,num_timepoints,num_classes)\n",
    "\n",
    "#     #get entropies for each image & each timepoint\n",
    "#     entropies_pred = np.ones([num_images_all,num_timepoints])\n",
    "#     entropies_pred[:] = np.nan\n",
    "\n",
    "#     for image in range(num_images_all):\n",
    "#         for tp in range(num_timepoints):\n",
    "#             entropies_pred[image,tp] = entropy(pred_reshaped[image,tp])\n",
    "\n",
    "#     # #for each image, determine the timepoint when entropy reaches threshold\n",
    "#     rt_thresh = np.ones(num_images_all)\n",
    "#     rt_thresh[:] = np.nan\n",
    "#     for image in range(num_images_all):\n",
    "#         for tp in range(num_timepoints):\n",
    "#             if entropies_pred[image,tp] <= entropy_thresh:\n",
    "#                 rt_thresh[image]=tp\n",
    "#                 break          \n",
    "\n",
    "#     #if it never reaches the threshold (nan in the array), replace by 8\n",
    "#     rt_thresh[np.isnan(rt_thresh)] = 8\n",
    "   \n",
    "#     return rt_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io as sio\n",
    "# from scipy import stats\n",
    "# from scipy.stats import entropy\n",
    "# from scipy import stats\n",
    "# rts_eeg_dict = sio.loadmat(os.path.join(main_path+'/rcnn-sat/','median_RT_subjects_5_35.mat'))\n",
    "# rts_eeg = np.transpose(rts_eeg_dict.get('RT'))\n",
    "# entropies = np.arange(0.01,0.11,0.01)\n",
    "# correlations = np.ones([len(entropies)])\n",
    "# correlations[:] = np.nan\n",
    "# rts_e = np.ones([len(entropies),len(test_images_paths)])\n",
    "# rts_e[:] = np.nan\n",
    "\n",
    "# for idx,e in enumerate(entropies):\n",
    "#     rts_e[idx,:] = get_RTs(test_images_paths,20,e)\n",
    "#     correlations[idx] = stats.spearmanr(np.squeeze(rts_e[idx,:]),rts_eeg)[0]\n",
    "    \n",
    "# best_entropy = entropies[np.argmax(correlations)]\n",
    "# rt_path = os.path.join(main_path+'/rcnn-sat/'+model_name,'reaction_time_entropy_th_{}'.format(np.round(best_entropy,2)))\n",
    "# np.save(rt_path,np.squeeze(rts_e[np.argmax(correlations)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.round(best_entropy,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rt_path = os.path.join(main_path+'/rcnn-sat/'+model_name,'reaction_time_entropy_th_{}'.format(np.round(best_entropy,2)))\n",
    "\n",
    "# np.save(rt_path,np.squeeze(rts_e[np.argmax(correlations)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations[np.argmax(correlations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
